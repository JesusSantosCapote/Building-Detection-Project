{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNJeLRTe9lDO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bf4937AGrgH"
      },
      "outputs": [],
      "source": [
        "!pip install awscli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25dM2KwIG-Ue"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from distutils.dir_util import copy_tree\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzeIGme1HJFk"
      },
      "outputs": [],
      "source": [
        "%mkdir Building_Detection\n",
        "%cd Building_Detection\n",
        "\n",
        "%mkdir data\n",
        "%cd data\n",
        "%mkdir train\n",
        "%mkdir val\n",
        "%mkdir train_raw_dataset\n",
        "%mkdir val_raw_dataset\n",
        "%cd train\n",
        "%mkdir images\n",
        "%mkdir annotations\n",
        "%cd ..\n",
        "%cd val\n",
        "%mkdir images\n",
        "%mkdir annotations\n",
        "%cd ..\n",
        "%cd ..\n",
        "\n",
        "path = os.getcwd()\n",
        "\n",
        "train_path = os.path.join(path, \"data\", \"train\")\n",
        "val_path = os.path.join(path, \"data\", \"val\")\n",
        "\n",
        "raw_train_path = os.path.join(path, \"data\", \"train_raw_dataset\")\n",
        "raw_val_path = os.path.join(path, \"data\", \"val_raw_dataset\")\n",
        "\n",
        "train_annot_path = os.path.join(train_path,\"annotations\")\n",
        "train_images_path = os.path.join(train_path, \"images\")\n",
        "\n",
        "val_annot_path = os.path.join(val_path,\"annotations\")\n",
        "val_images_path = os.path.join(val_path, \"images\")\n",
        "\n",
        "\n",
        "checkpoint_drive_path = \"/content/drive/MyDrive/model_checkpoint\"\n",
        "\n",
        "best_model_path = os.path.join(checkpoint_drive_path, \"best.pt \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5ViBtz_uDKJ"
      },
      "source": [
        "#Train Dataset Download Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72rPN9O7s1kp"
      },
      "outputs": [],
      "source": [
        "!aws s3 cp s3://spacenet-dataset/Hosted-Datasets/fmow/fmow-rgb/train/stadium /content/Building_Detection/data/train_raw_dataset/ --recursive --no-sign-request"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfDFVGq7_PNF"
      },
      "source": [
        "#Validation Dataset Donwload Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15si16C5_klR"
      },
      "outputs": [],
      "source": [
        "!aws s3 ls s3://spacenet-dataset/Hosted-Datasets/fmow/fmow-rgb/ --no-sign-request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TC9hi1TPeSk"
      },
      "outputs": [],
      "source": [
        "!aws s3 cp s3://spacenet-dataset/Hosted-Datasets/fmow/fmow-rgb/val/stadium /content/Building_Detection/data/val_raw_dataset/ --recursive --no-sign-request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swc3ytL1ReUK"
      },
      "outputs": [],
      "source": [
        "def check_bbox(json_dict):\n",
        "  for elem in json_dict[\"bounding_boxes\"]:\n",
        "    x_min, y_min, x_max, y_max = elem['box']\n",
        "\n",
        "    if x_min > x_max or y_min > y_max:\n",
        "      return False\n",
        "\n",
        "  return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibsrKCldQvDw"
      },
      "outputs": [],
      "source": [
        "def move_raw_datasets(actual_folder, img_destiny, annot_destiny, amount):\n",
        "  folders = os.listdir(actual_folder)[0:amount]\n",
        "  for directory in folders:\n",
        "    source = os.path.join(actual_folder, directory)\n",
        "\n",
        "    for file in os.listdir(source):\n",
        "      if file.endswith(\".json\"):\n",
        "        shutil.move(os.path.join(source, file), annot_destiny)\n",
        "        image_name = file.split('.')[0]\n",
        "        image_name += \".jpg\"\n",
        "\n",
        "        try:\n",
        "          shutil.move(os.path.join(source, image_name), img_destiny)\n",
        "        except:\n",
        "          continue\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKF1ENFEcqLg"
      },
      "outputs": [],
      "source": [
        "def delete_bad_data(images_dir, annot_dir):\n",
        "  data_to_delete = []\n",
        "  number_of_deleted = 0\n",
        "\n",
        "  for file in os.listdir(annot_dir):\n",
        "    with open(os.path.join(annot_dir, file)) as f:\n",
        "      json_dict = json.load(f)\n",
        "\n",
        "      for elem in json_dict[\"bounding_boxes\"]:\n",
        "        x_min, y_min, x_max, y_max = elem[\"box\"]\n",
        "        if abs(x_min - x_max) < 10e-8 or abs(y_min - y_max) < 10e-8:\n",
        "          annot2delete = os.path.join(annot_dir, file)\n",
        "          image_name = file.split('.')[0]\n",
        "          image_name += \".jpg\"\n",
        "          image2delete = os.path.join(images_dir, image_name)\n",
        "          data_to_delete.append(annot2delete)\n",
        "          data_to_delete.append(image2delete)\n",
        "\n",
        "  for file in data_to_delete:\n",
        "    os.remove(file)\n",
        "    number_of_deleted += 1\n",
        "\n",
        "  return number_of_deleted\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZQLseLQRbPz"
      },
      "outputs": [],
      "source": [
        "move_raw_datasets(raw_train_path, train_images_path, train_annot_path, 200)\n",
        "move_raw_datasets(raw_val_path, val_images_path, val_annot_path, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejgwrHPyOYS3"
      },
      "outputs": [],
      "source": [
        "print(delete_bad_data(train_images_path, train_annot_path))\n",
        "print(delete_bad_data(val_images_path, val_annot_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZE3FQV0Ki7o"
      },
      "outputs": [],
      "source": [
        "import albumentations\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from torchvision import transforms as torchtrans\n",
        "\n",
        "\n",
        "def resize_image(img_arr, bboxes, h, w):\n",
        "    \"\"\"\n",
        "    :param img_arr: original image as a numpy array\n",
        "    :param bboxes: bboxes as numpy array where each row is 'x_min', 'y_min', 'x_max', 'y_max', \"class_id\"\n",
        "    :param h: resized height dimension of image\n",
        "    :param w: resized weight dimension of image\n",
        "    :return: dictionary containing {image:transformed, bboxes:['x_min', 'y_min', 'x_max', 'y_max', \"class_id\"]}\n",
        "    \"\"\"\n",
        "    # create resize transform pipeline\n",
        "    transform = albumentations.Compose(\n",
        "        [albumentations.Resize(height=h, width=w, always_apply=True)],\n",
        "        bbox_params=albumentations.BboxParams(format='pascal_voc'))\n",
        "\n",
        "    for box in bboxes:\n",
        "      if box[0] > box[2]:\n",
        "        temp = box[0]\n",
        "        box[0] = box[2]\n",
        "        box[2] = temp\n",
        "\n",
        "      if box[1] > box[3]:\n",
        "        temp = box[1]\n",
        "        box[1] = box[3]\n",
        "        box[3] = temp\n",
        "\n",
        "    transformed = transform(image=img_arr, bboxes=bboxes)\n",
        "\n",
        "    return transformed\n",
        "\n",
        "\n",
        "# Function to visualize bounding boxes in the image\n",
        "def plot_img_bbox(img, target, classes):\n",
        "  # plot the image and bboxes\n",
        "  # Bounding boxes are defined as follows: x-min y-min width height\n",
        "  if \"scores\" not in target.keys():\n",
        "    scores = [100 for i in range(len(target['boxes']))]\n",
        "  else:\n",
        "    scores = target['scores']\n",
        "\n",
        "  fig, a = plt.subplots(1,1)\n",
        "  fig.set_size_inches(5,5)\n",
        "  try:\n",
        "    a.imshow(img)\n",
        "  except Exception:\n",
        "    a.imshow(img.T)\n",
        "  for box, label, score in zip(target['boxes'], target['labels'], scores):\n",
        "    x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
        "    print(x, y, width, height)\n",
        "    rect = patches.Rectangle(\n",
        "      (x, y),\n",
        "      width, height,\n",
        "      linewidth = 2,\n",
        "      edgecolor = 'r',\n",
        "      facecolor = 'none',\n",
        "    )\n",
        "    a.text(x, y, f\"{classes[label]}:{score}\")\n",
        "    # Draw the bounding box on top of the image\n",
        "    a.add_patch(rect)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def copy_raw_datasets(actual_folder, img_destiny, annot_destiny):\n",
        "\n",
        "  for directory in os.listdir(actual_folder):\n",
        "    source = os.path.join(actual_folder, directory)\n",
        "    if directory.endswith(\".json\"):\n",
        "      shutil.copy(source, annot_destiny)\n",
        "    else:\n",
        "      shutil.copy(source, img_destiny)\n",
        "\n",
        "\n",
        "# Send train=True for training transforms and False for val/test transforms\n",
        "def get_transform(train):\n",
        "  if train:\n",
        "    return albumentations.Compose(\n",
        "      [\n",
        "        albumentations.HorizontalFlip(0.5),\n",
        "        # ToTensorV2 converts image to pytorch tensor without div by 255\n",
        "        ToTensorV2(p=1.0)\n",
        "      ],\n",
        "      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n",
        "    )\n",
        "  else:\n",
        "    return albumentations.Compose(\n",
        "      [ToTensorV2(p=1.0)],\n",
        "      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n",
        "    )\n",
        "\n",
        "\n",
        "# function to convert a torchtensor back to PIL image\n",
        "def torch_to_pil(img):\n",
        "  return torchtrans.ToPILImage()(img).convert('RGB')\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FmSo1GtLW3b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# we create a Dataset class which has a __getitem__ function and a __len__ function\n",
        "class BuildingImageDataset(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, images_dir, annot_dir, width, height, transforms=None):\n",
        "    self.transforms = transforms\n",
        "    self.images_dir = images_dir\n",
        "    self.annot_dir = annot_dir\n",
        "    self.height = height\n",
        "    self.width = width\n",
        "\n",
        "    # sorting the images for consistency\n",
        "    # To get images, the extension of the filename is checked to be jpg\n",
        "    self.imgs = [image for image in sorted(os.listdir(self.images_dir)) if image[-4:]=='.jpg']\n",
        "\n",
        "    # classes: 0 index is reserved for background\n",
        "    self.classes = ['background', 'stadium']\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_name = self.imgs[idx]\n",
        "    image_path = os.path.join(self.images_dir, img_name)\n",
        "\n",
        "    # reading the images and converting them to correct size and color\n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "\n",
        "    # annotation file\n",
        "    annot_filename = img_name[:-4] + '.json'\n",
        "    annot_file_path = os.path.join(self.annot_dir, annot_filename)\n",
        "\n",
        "    image_info = []\n",
        "\n",
        "    with open(annot_file_path) as f:\n",
        "\n",
        "      json_dict = json.load(f)\n",
        "\n",
        "      for elem in json_dict[\"bounding_boxes\"]:\n",
        "        label = self.classes.index(elem[\"category\"])\n",
        "\n",
        "        box = elem['box']\n",
        "\n",
        "        xmin = max(0, int(float(box[0])))\n",
        "        ymin = max(0, int(float(box[1])))\n",
        "        xmax = min(int(json_dict[\"img_width\"]), int(float(box[2])))\n",
        "        ymax = min(int(json_dict[\"img_height\"]), int(float(box[3])))\n",
        "\n",
        "        if xmin >= xmax:\n",
        "          temp = xmin\n",
        "          xmin = xmax\n",
        "          xmax = temp\n",
        "\n",
        "        if ymin >= ymax:\n",
        "          temp = ymin\n",
        "          ymin = ymax\n",
        "          ymax = temp\n",
        "\n",
        "        image_info.append([xmin, ymin, xmax, ymax, label])\n",
        "\n",
        "      f.close()\n",
        "\n",
        "    image_info = np.array(image_info)\n",
        "\n",
        "    transformed_dict = resize_image(img_rgb, image_info, 224, 224)\n",
        "\n",
        "    # contains the image as array\n",
        "    img_res = np.asarray(transformed_dict[\"image\"])\n",
        "\n",
        "    # diving by 255\n",
        "    img_res /= 255.0\n",
        "\n",
        "    boxes = []\n",
        "    labels = []\n",
        "\n",
        "    for elem in transformed_dict[\"bboxes\"]:\n",
        "      boxes.append([elem[0], elem[1], elem[2], elem[3]])\n",
        "      labels.append(elem[4])\n",
        "\n",
        "    # convert boxes into a torch.Tensor\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "\n",
        "    # getting the areas of the boxes\n",
        "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "    # suppose all instances are not crowd\n",
        "    iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "\n",
        "    labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"area\"] = area\n",
        "    target[\"iscrowd\"] = iscrowd\n",
        "    image_id = torch.tensor([idx])\n",
        "    target[\"image_id\"] = image_id\n",
        "\n",
        "    if self.transforms:\n",
        "      sample = self.transforms(image = img_res,\n",
        "                                bboxes = target['boxes'],\n",
        "                                labels = labels)\n",
        "      img_res = sample['image']\n",
        "      target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "\n",
        "    return img_res, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfmkjHPV_YKR"
      },
      "outputs": [],
      "source": [
        "classes = ['background', 'stadium']\n",
        "# use our dataset and defined transformations\n",
        "train_dataset = BuildingImageDataset(train_images_path, train_annot_path, 480, 480, transforms=get_transform(train=False))\n",
        "val_dataset = BuildingImageDataset(val_images_path, val_annot_path, 480, 480, transforms=get_transform(train=False))\n",
        "\n",
        "# define training and validation data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  train_dataset,\n",
        "  batch_size=10,\n",
        "  shuffle=True,\n",
        "  num_workers=0,\n",
        "  collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "print(len(train_loader))\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "  val_dataset,\n",
        "  batch_size=10,\n",
        "  shuffle=False,\n",
        "  num_workers=0,\n",
        "  collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "print(len(valid_loader))\n",
        "\n",
        "num_classes = 2 # one class (class 0) is dedicated to the \"background\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2eDg-X1Lwzn"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "def get_object_detection_model(num_classes):\n",
        "  # load a model pre-trained pre-trained on COCO\n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "  # get number of input features for the classifier\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  # replace the pre-trained head with a new one\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWvijKZ1HuFf"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7sS6lTcM6iX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.datasets import VOCDetection\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from pprint import pprint\n",
        "import os\n",
        "import time\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\"\"\"Let the training begin!\"\"\"\n",
        "checkpoint_folder = \"/content/drive/MyDrive/model_checkpoint\"\n",
        "checkpoint_path = os.path.join(checkpoint_folder, \"checkpoint.pt \")\n",
        "best_model_path = os.path.join(checkpoint_folder, \"best.pt \")\n",
        "\n",
        "\n",
        "writer = SummaryWriter()\n",
        "metric = MeanAveragePrecision()\n",
        "\n",
        "def train(model, optimizer, train_loader, valid_loader, lr_scheduler):\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    model.to(device)\n",
        "    num_epochs = 20\n",
        "    start_epoch = 0\n",
        "    best_mAP = -1\n",
        "\n",
        "    if 'checkpoint.pt' in os.listdir(checkpoint_folder):\n",
        "        print('..............Checkpoint Loaded................')\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        best_mAP = checkpoint['best_mAP']\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        running_loss = 0.0\n",
        "        train_total_of_samples = 0\n",
        "        val_total_of_samples = 0\n",
        "        mAP_sum = 0.0\n",
        "\n",
        "        print(f\"........................Starting Epoch: {epoch}...........................\")\n",
        "        model.train()\n",
        "        print('.............Training................')\n",
        "\n",
        "        start_time = time.time()\n",
        "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "            images = [image.to(device) for image in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            running_loss += losses.item()\n",
        "            train_total_of_samples += len(targets)\n",
        "            writer.add_scalar('Training Loss', losses.item(), epoch * len(train_loader) + batch_idx)\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 10 == 4:\n",
        "              elapsed_time = time.time() - start_time\n",
        "              avg_loss = running_loss / train_total_of_samples\n",
        "              print(f'Batch[{batch_idx+1}/{len(train_loader)}], Avg. Loss: {avg_loss:.4f}, Time: {elapsed_time:.2f}s')\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        print('...............Validation.................')\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (images, targets) in enumerate(valid_loader):\n",
        "                images = [image.to(device) for image in images]\n",
        "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "                predict = model(images)\n",
        "                metric.update(predict, targets)\n",
        "                pprint(metric.compute())\n",
        "                mAP = metric.compute()['map'].item()\n",
        "                mAP_sum += mAP\n",
        "                val_total_of_samples += len(targets)\n",
        "                writer.add_scalar('Validation mAP', mAP, epoch * len(valid_loader) + batch_idx)\n",
        "\n",
        "                if batch_idx % 10 == 4:\n",
        "                  elapsed_time = time.time() - start_time\n",
        "                  avg_mAP = mAP_sum / val_total_of_samples\n",
        "                  print(f'Batch[{batch_idx+1}/{len(valid_loader)}], Avg. mAP: {avg_mAP:.4f}, Time: {elapsed_time:.2f}s')\n",
        "\n",
        "                if mAP > best_mAP:\n",
        "                    best_mAP = mAP\n",
        "                    # Save the best model\n",
        "                    torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "\n",
        "        # Save a checkpoint of the model and optimizer\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'best_mAP': best_mAP\n",
        "        }\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        print(\"..............Checkpoint Saved...............\")\n",
        "        # Print the loss and accuracy for the epoch\n",
        "        print(f'Ending Epoch {epoch}/{num_epochs}, Epoch Loss: {running_loss/len(train_loader.dataset):.4f}, Epoch mAP: {mAP_sum/ len(valid_loader.dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZFd7ng4NH6j"
      },
      "outputs": [],
      "source": [
        "model = get_object_detection_model(num_classes)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "  optimizer,\n",
        "  step_size=3,\n",
        "  gamma=0.1\n",
        ")\n",
        "\n",
        "\n",
        "train(model, optimizer, train_loader, valid_loader, lr_scheduler)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "69c2e9a108d15bc249090ca8e20dfae7e8e223d98139d5f8c616b824eb6b2d7a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
