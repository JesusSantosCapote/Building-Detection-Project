# -*- coding: utf-8 -*-
"""pytorch_version.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lBbTvYDQnrQunovtQs5Kc7SRvk9JiCS2
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install awscli

import os
import shutil
from distutils.dir_util import copy_tree

# Commented out IPython magic to ensure Python compatibility.
# %mkdir ML_Project
# %cd ML_Project

!git clone https://github.com/pytorch/vision.git
# %cd vision
!git checkout v0.3.0
# %cd ..
!pwd
!cp vision/references/detection/utils.py ./
!cp vision/references/detection/transforms.py ./
# %cd ..
# %cd drive
# %cd MyDrive
# %cd Colab\ Notebooks
!cp coco_eval.py /content/ML_Project/
# %cd ..
# %cd ..
# %cd ..
# %cd ML_Project
!cp vision/references/detection/engine.py ./
!cp vision/references/detection/coco_utils.py ./

# %mkdir data
# %cd data
# %mkdir train
# %mkdir val
# %mkdir train_raw_dataset
# %mkdir val_raw_dataset
# %cd train
# %mkdir images
# %mkdir annotations
# %cd ..
# %cd val
# %mkdir images
# %mkdir annotations
# %cd ..
path = os.getcwd()
train_path = os.path.join(path, "train")
val_path = os.path.join(path, "val")

a = "/content/drive/MyDrive/model_checkpoint"

# Commented out IPython magic to ensure Python compatibility.
# %cd train_raw_dataset
!aws s3 cp s3://spacenet-dataset/Hosted-Datasets/fmow/fmow-rgb/train/airport/airport_9 . --recursive --no-sign-request
# %cd ..

# Commented out IPython magic to ensure Python compatibility.
# %cd val_raw_dataset
!aws s3 cp s3://spacenet-dataset/Hosted-Datasets/fmow/fmow-rgb/val/airport/airport_9 . --recursive --no-sign-request
# %cd ..

def copy_raw_datasets(actual_folder, img_destiny, annot_destiny):

  for directory in os.listdir(actual_folder):
    source = os.path.join(actual_folder, directory)
    if directory.endswith(".json"):
      shutil.copy(source, annot_destiny)
    else:
      shutil.copy(source, img_destiny)

actual_path = os.getcwd()

raw_train_path = os.path.join(actual_path, "train_raw_dataset")
raw_val_path = os.path.join(actual_path, "val_raw_dataset")

train_annot_path = os.path.join(train_path,"annotations")
train_images_path = os.path.join(train_path, "images")

val_annot_path = os.path.join(val_path,"annotations")
val_images_path = os.path.join(val_path, "images")

copy_raw_datasets(raw_train_path, train_images_path, train_annot_path)
copy_raw_datasets(raw_val_path, val_images_path, val_annot_path)

def review_order(img, lbl):
  bad_match = False
  for i in range(len(img)):
    list1 = img[i].split("/")
    list2 = lbl[i].split("/")

    temp1 = list1[-1].split('.')[0]
    temp2 = list2[-1].split('.')[0]

    if temp1 != temp2: bad_match = True

  return bad_match

img_paths = sorted([os.path.join(train_images_path, fname) for fname in os.listdir(train_images_path) if fname.endswith(".jpg")])
label_paths = sorted([os.path.join(train_annot_path, fname) for fname in os.listdir(train_annot_path) if fname.endswith(".json")])
bad_order = review_order(img_paths, label_paths)
bad_order

"""# Formula Trinity ü§ù DUCSS - Object Detection Workshop!

Hello Everyone!

Today we will be going through how to make a traffic cone detector using [PyTorch](https://pytorch.org/).

This notebook was adapted from the one in [Fine-tuning Faster-RCNN using pytorch](https://www.kaggle.com/yerramvarun/fine-tuning-faster-rcnn-using-pytorch/notebook)

## Installs and Imports

Install and import PyTorch along with a few helper libraries

Let's install some dependencies and clone the [TorchVision Repo](https://github.com/pytorch/vision) so we can use some helper files

Lets import the libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd ..

# basic python and ML Libraries
import os
import random
import numpy as np
import pandas as pd

# for ignoring warnings
import warnings
warnings.filterwarnings('ignore')

# We will be reading images using OpenCV
import cv2

# matplotlib for visualization
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# torchvision libraries
import torch
import torchvision
from torchvision import transforms as torchtrans  
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

# helper libraries
from engine import evaluate
import utils
import transforms as T
import json
from sklearn import preprocessing
import math
import sys

# for image augmentations
import albumentations
from albumentations.pytorch.transforms import ToTensorV2
from PIL import Image

"""## The Dataset

Having a good dataset is essential to training an accurate model. However, just possessing the dataset is not enough. We must pre-process it so that it can be fed to the neural network - and then understood by it.

Finding an existing dataset can sometimes be very difficult. When searching for your dataset you may run into the following problems:

* **Broken links**: even if someone has open-sourced something, links they previously had to their datasets often end up becoming stale.
* **Poor Open-Source Practice**: some "open-source" dataset providers will insist on you contributing to the dataset to be provided access.

However, keep searching! Here's some tricks:

* **Search specific websites**: When searching on Google, you can search specifically on GitHub by searching something like: `cone detection dataset site:github.com`.
* **Reading other people's code**: Try to find the code of someone else that worked on a similar problem, and see if you can find out what dataset they used! Such as searching: `image classification on traffic cones` and trying to find other notebooks.


We found the dataset for this worksop by scouring GitHub repos. It contains 123 annotated images in the training set and 

Let's download it!
"""

def resize_image(img_arr, bboxes, h, w):
    """
    :param img_arr: original image as a numpy array
    :param bboxes: bboxes as numpy array where each row is 'x_min', 'y_min', 'x_max', 'y_max', "class_id"
    :param h: resized height dimension of image
    :param w: resized weight dimension of image
    :return: dictionary containing {image:transformed, bboxes:['x_min', 'y_min', 'x_max', 'y_max', "class_id"]}
    """
    # create resize transform pipeline
    transform = albumentations.Compose(
        [albumentations.Resize(height=h, width=w, always_apply=True)],
        bbox_params=albumentations.BboxParams(format='pascal_voc'))

    transformed = transform(image=img_arr, bboxes=bboxes)

    return transformed

# Label Encoder
le = preprocessing.LabelEncoder()
le.fit([_, 'airport'])

# we create a Dataset class which has a __getitem__ function and a __len__ function
class BuildingImageDataset(torch.utils.data.Dataset):

  def __init__(self, images_dir, annot_dir, width, height, transforms=None):
    self.transforms = transforms
    self.images_dir = images_dir
    self.annot_dir = annot_dir
    self.height = height
    self.width = width
    
    # sorting the images for consistency
    # To get images, the extension of the filename is checked to be jpg
    self.imgs = [image for image in sorted(os.listdir(self.images_dir)) if image[-4:]=='.jpg']
    
    # classes: 0 index is reserved for background
    self.classes = [_, 'airport']

  def __getitem__(self, idx):
    img_name = self.imgs[idx]
    image_path = os.path.join(self.images_dir, img_name)

    # reading the images and converting them to correct size and color    
    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)

    # annotation file
    annot_filename = img_name[:-4] + '.json'
    annot_file_path = os.path.join(self.annot_dir, annot_filename)
    
    image_info = []

    with open(annot_file_path) as f:

      json_dict = json.load(f)

      for elem in json_dict["bounding_boxes"]:
        label = self.classes.index(elem["category"])

        box = elem['box']

        xmin = max(0, int(float(box[0])))
        ymin = max(0, int(float(box[1])))
        xmax = min(int(json_dict["img_width"]), int(float(box[2])))
        ymax = min(int(json_dict["img_height"]), int(float(box[3])))

        image_info.append([xmin, ymin, xmax, ymax, label])

      f.close()

    image_info = np.array(image_info)

    transformed_dict = resize_image(img_rgb, image_info, 224, 224)

    # contains the image as array
    img_res = np.asarray(transformed_dict["image"])

    # diving by 255
    img_res /= 255.0

    boxes = []
    labels = []
    
    for elem in transformed_dict["bboxes"]:
      print(elem)
      boxes.append([elem[0], elem[1], elem[2], elem[3]])
      labels.append(elem[4])
    
    # convert boxes into a torch.Tensor
    boxes = torch.as_tensor(boxes, dtype=torch.float32)
    
    # getting the areas of the boxes
    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])

    # suppose all instances are not crowd
    iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
    
    labels = torch.as_tensor(labels, dtype=torch.int64)

    target = {}
    target["boxes"] = boxes
    target["labels"] = labels
    target["area"] = area
    target["iscrowd"] = iscrowd
    image_id = torch.tensor([idx])
    target["image_id"] = image_id

    if self.transforms:
      sample = self.transforms(image = img_res,
                                bboxes = target['boxes'],
                                labels = labels)
      img_res = sample['image']
      target['boxes'] = torch.Tensor(sample['bboxes'])
        
    return img_res, target

  def __len__(self):
    return len(self.imgs)


# check dataset
dataset = BuildingImageDataset(train_images_path, train_annot_path, 224, 224)
print('Length of dataset:', len(dataset), '\n')

# getting the image and target for a test index.  Feel free to change the index.
img, target = dataset[3]
print('Image shape:', img.shape)
print('Label example:', target)

"""# Visualization

Let's make some a helper function to view our data
"""

# Function to visualize bounding boxes in the image
def plot_img_bbox(img, target):
  # plot the image and bboxes
  # Bounding boxes are defined as follows: x-min y-min width height
  if "scores" not in target.keys():
    scores = [100 for i in range(len(target['boxes']))]
  else:
    scores = target['scores']
  
  fig, a = plt.subplots(1,1)
  fig.set_size_inches(5,5)
  try:
    a.imshow(img)
  except Exception:
    a.imshow(img.T)
  for box, label, score in zip(target['boxes'], target['labels'], scores):
    x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]
    print(x, y, width, height)
    rect = patches.Rectangle(
      (x, y),
      width, height,
      linewidth = 2,
      edgecolor = 'r',
      facecolor = 'none',
    )
    a.text(x, y, f"{le.inverse_transform([label])}:{score}")
    # Draw the bounding box on top of the image
    a.add_patch(rect)
  plt.show()
    
# plotting the image with bboxes. Feel free to change the index
img, target = dataset[1]
plot_img_bbox(img, target)

"""# Augmentations

This is where we can apply augmentations to the image. 

The augmentations to object detection vary from normal augmentations becuase here we need to ensure that, bbox still aligns with the object correctly after transforming.

Here we are doing a random flip transform.
"""

# Send train=True for training transforms and False for val/test transforms
def get_transform(train):
  if train:
    return albumentations.Compose(
      [
        albumentations.HorizontalFlip(0.5),
        # ToTensorV2 converts image to pytorch tensor without div by 255
        ToTensorV2(p=1.0) 
      ],
      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}
    )
  else:
    return albumentations.Compose(
      [ToTensorV2(p=1.0)],
      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}
    )

"""# Dataloaders

Make a loader for feeding our data into the neural network

Now lets prepare the datasets and dataloaders for training and testing.
"""

# use our dataset and defined transformations
dataset = BuildingImageDataset(train_images_path, train_annot_path, 480, 480, transforms=get_transform(train=False))
dataset_test = BuildingImageDataset(val_images_path, val_annot_path, 480, 480, transforms=get_transform(train=False))

# define training and validation data loaders
data_loader = torch.utils.data.DataLoader(
  dataset,
  batch_size=10,
  shuffle=True,
  num_workers=4,
  collate_fn=utils.collate_fn,
)

data_loader_test = torch.utils.data.DataLoader(
  dataset_test,
  batch_size=10,
  shuffle=False,
  num_workers=4,
  collate_fn=utils.collate_fn,
)

"""# Pre-trained Model"""

def get_object_detection_model(num_classes):
  # load a model pre-trained pre-trained on COCO
  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
  # get number of input features for the classifier
  in_features = model.roi_heads.box_predictor.cls_score.in_features
  # replace the pre-trained head with a new one
  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) 
  return model

"""# Training

Let's prepare the model for training
"""

#Save and Load the Model checkpoints

checkpoint_path = "/content/drive/MyDrive/model_checkpoint"

def save_ckp(state, is_best):
    f_path = os.path.join(checkpoint_path, 'checkpoint.pt') 
    torch.save(state, f_path)
    if is_best:
        best_fpath = os.path.join(checkpoint_path, 'best_model.pt') 
        shutil.copyfile(f_path, best_fpath)


def load_ckp(checkpoint_fpath):
    checkpoint = torch.load(checkpoint_fpath)
    # model.load_state_dict(checkpoint['state_dict'])
    # optimizer.load_state_dict(checkpoint['optimizer'])
    return checkpoint

# train on gpu if available
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

num_classes = 2 # one class (class 0) is dedicated to the "background"

checkpoint_path = "/content/drive/MyDrive/model_checkpoint"

checkpoint = None

if 'best_model.pt' in os.listdir(checkpoint_path):
  checkpoint = load_ckp(os.path.join(checkpoint_path, 'best_model.pt' ))

initial_epoch = 0

# get the model using our helper function
model = get_object_detection_model(num_classes)

# move model to the right device
model.to(device)


if checkpoint:
  model.load_state_dict(checkpoint['state_dict'])
  initial_epoch = checkpoint['epoch']

# construct an optimizer
params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)

if checkpoint:
  optimizer.load_state_dict(checkpoint['optimizer'])

# and a learning rate scheduler which decreases the learning rate by
# 10x every 3 epochs
lr_scheduler = torch.optim.lr_scheduler.StepLR(
  optimizer,
  step_size=3,
  gamma=0.1
)

if checkpoint:
  lr_scheduler.load_state_dict(checkpoint['scheduler'])

"""Let the training begin!"""

#Training method
def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):
    model.train()
    metric_logger = utils.MetricLogger(delimiter="  ")
    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))
    header = 'Epoch: [{}]'.format(epoch)
    print("llegue")

    lr_scheduler = None
    if epoch == 0:
        warmup_factor = 1. / 1000
        warmup_iters = min(1000, len(data_loader) - 1)

        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)

    for images, targets in metric_logger.log_every(data_loader, print_freq, header):
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        loss_dict = model(images, targets)

        losses = sum(loss for loss in loss_dict.values())

        # reduce losses over all GPUs for logging purposes
        loss_dict_reduced = utils.reduce_dict(loss_dict)
        losses_reduced = sum(loss for loss in loss_dict_reduced.values())

        loss_value = losses_reduced.item()

        if not math.isfinite(loss_value):
            print("Loss is {}, stopping training".format(loss_value))
            print(loss_dict_reduced)
            sys.exit(1)

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        if lr_scheduler is not None:
            lr_scheduler.step()

        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)
        metric_logger.update(lr=optimizer.param_groups[0]["lr"])
        return losses_reduced

# training for 5 epochs
num_epochs = 5
train_loss = []
best_loss = math.inf

for epoch in range(initial_epoch, num_epochs):
    # training for one epoch
    epoch_loss = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)
    train_loss.append(epoch_loss.item())

    is_best = epoch_loss.item() < best_loss

    if is_best:
      best_loss = epoch_loss.item()

   

    # update the learning rate
    lr_scheduler.step()

    checkpoint = {
      'epoch': epoch + 1,
      'state_dict': model.state_dict(),
      'optimizer': optimizer.state_dict(),
      'scheduler': lr_scheduler.state_dict()
    }

    save_ckp(checkpoint, is_best)

    # evaluate on the test dataset
    statistics = evaluate(model, data_loader, device=device)

"""# Filtering the outputs

Our model predicts a lot of bounding boxes per image, so take out the overlapping ones, we will use **Non Max Suppression** (NMS). If you want to brush up on that, check [this](https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c) out.

Torchvision provides us a utility to apply NMS to our predictions, lets build a function `apply_nms` using that.
"""

# the function takes the original prediction and the iou threshold.
def apply_nms(orig_prediction, iou_thresh=0.3):
  # torchvision returns the indices of the bboxes to keep
  keep = torchvision.ops.nms(orig_prediction['boxes'].cpu(), orig_prediction['scores'].cpu(), iou_thresh)
  
  final_prediction = orig_prediction
  final_prediction['boxes'] = final_prediction['boxes'].cpu()[keep]
  final_prediction['scores'] = final_prediction['scores'].cpu()[keep]
  final_prediction['labels'] = final_prediction['labels'].cpu()[keep]
  
  return final_prediction

# function to convert a torchtensor back to PIL image
def torch_to_pil(img):
  return torchtrans.ToPILImage()(img).convert('RGB')

# Commented out IPython magic to ensure Python compatibility.
# %cd data
# %cd val
# %cd images
!ls

"""# Testing our Model

Now lets take an image from the test set and try to predict on it
"""

# pick one image from the test set
img, target = dataset[7]

# put the model in evaluation mode
model.eval()
with torch.no_grad():
  prediction = model([img.to(device)])[0]


print('MODEL OUTPUT\n')
nms_prediction = apply_nms(prediction, iou_thresh=0.01)

plot_img_bbox(torch_to_pil(img), nms_prediction)